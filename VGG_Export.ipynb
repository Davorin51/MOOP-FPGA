{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c8e130-607c-4d5d-9767-e083caba311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded from 'model_vgg8.pth'.\n",
      "Max value in quantized input: 255.0\n",
      "Exporting model to QONNX format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to ONNX file: ./export/part1.onnx\n",
      "Cleaning ONNX model with qonnx_cleanup...\n",
      "Cleaned ONNX model saved to: ./export/part1_clean.onnx\n",
      "Converting QONNX to FINN model...\n",
      "FINN model saved to: ./export/ready_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davorin/Documents/finn/deps/qonnx/src/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 14. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "# Pretpostavimo da je ista arhitektura definirana\n",
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU, QuantMaxPool2d\n",
    "\n",
    "# Definicija iste arhitekture (morate imati isto definirane težine)\n",
    "class QuantVGG8(torch.nn.Module):\n",
    "    def __init__(self, in_channels=3, bit_width=4, num_classes=10, img_size=32):\n",
    "        super(QuantVGG8, self).__init__()\n",
    "        self.features = torch.nn.Sequential(\n",
    "            QuantConv2d(in_channels, 16, kernel_size=3, padding=1, weight_bit_width=bit_width),\n",
    "            QuantReLU(bit_width=bit_width),\n",
    "            QuantMaxPool2d(kernel_size=2),\n",
    "            QuantConv2d(16, 32, kernel_size=3, padding=1, weight_bit_width=bit_width),\n",
    "            QuantReLU(bit_width=bit_width),\n",
    "            QuantMaxPool2d(kernel_size=2),\n",
    "            QuantConv2d(32, 64, kernel_size=3, padding=1, weight_bit_width=bit_width),\n",
    "            QuantReLU(bit_width=bit_width),\n",
    "            QuantMaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        feature_map_size = img_size // 8\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            QuantLinear(64 * (feature_map_size**2), 128, bias=True, weight_bit_width=bit_width),\n",
    "            QuantReLU(bit_width=bit_width),\n",
    "            QuantLinear(128, num_classes, bias=True, weight_bit_width=bit_width)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def asymmetric_quantize(arr, num_bits=8):\n",
    "    qmin = 0\n",
    "    qmax = 2**num_bits - 1\n",
    "    beta = np.min(arr)\n",
    "    alpha = np.max(arr)\n",
    "    scale = (alpha - beta) / qmax if (alpha-beta)!=0 else 1.0\n",
    "    zero_point = np.clip(-beta/scale, 0, qmax).round().astype(np.int8)\n",
    "    quantized_arr = np.clip(np.round(arr / scale + zero_point), qmin, qmax).astype(np.float32)\n",
    "    return quantized_arr\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Inicijalizacija modela\n",
    "    model = QuantVGG8(in_channels=3, bit_width=4, num_classes=10, img_size=32)\n",
    "    model.load_state_dict(torch.load(\"model_vgg8.pth\", map_location=torch.device('cpu')))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(\"Model loaded from 'model_vgg8.pth'.\")\n",
    "\n",
    "    # Postavke za export\n",
    "    root_dir = \"./export\"\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    onnx_filename = os.path.join(root_dir, \"part1.onnx\")\n",
    "    onnx_clean_filename = os.path.join(root_dir, \"part1_clean.onnx\")\n",
    "    finn_filename = os.path.join(root_dir, \"ready_finn.onnx\")\n",
    "\n",
    "    # Kreiramo dummy ulaz – primjer za CIFAR-10 (3 kanala, 32x32)\n",
    "    dummy_np = np.random.rand(1, 3, 32, 32).astype(np.float32)\n",
    "    dummy_np = asymmetric_quantize(dummy_np, num_bits=8)\n",
    "    print(\"Max value in quantized input:\", np.max(dummy_np))\n",
    "    scale = 1.0\n",
    "    input_t = torch.from_numpy(dummy_np * scale)\n",
    "\n",
    "    print(\"Exporting model to QONNX format...\")\n",
    "    try:\n",
    "        export_qonnx(model, export_path=onnx_filename, input_t=input_t)\n",
    "        print(f\"Model exported to ONNX file: {onnx_filename}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during ONNX export:\", e)\n",
    "\n",
    "    print(\"Cleaning ONNX model with qonnx_cleanup...\")\n",
    "    try:\n",
    "        qonnx_cleanup(onnx_filename, out_file=onnx_clean_filename)\n",
    "        print(f\"Cleaned ONNX model saved to: {onnx_clean_filename}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error during ONNX model cleanup:\", e)\n",
    "\n",
    "    print(\"Converting QONNX to FINN model...\")\n",
    "    try:\n",
    "        model_wrapper = ModelWrapper(onnx_clean_filename)\n",
    "        model_wrapper = model_wrapper.transform(ConvertQONNXtoFINN())\n",
    "        model_wrapper.save(finn_filename)\n",
    "        print(\"FINN model saved to:\", finn_filename)\n",
    "    except Exception as e:\n",
    "        print(\"Error during FINN conversion:\", e)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b5ccd4-6cab-4820-919c-9467613881c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
